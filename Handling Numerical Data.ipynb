{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd172fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature:  [[ 1  1]\n",
      " [ 2  4]\n",
      " [34  8]]\n",
      "[[0.         0.        ]\n",
      " [0.03030303 0.42857143]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Handling_Numerical_Data\n",
    "\n",
    "# Rescaling a feature\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Create a feature\n",
    "feature = np.array([[1,1],[2,4],[34,8]])\n",
    "print(\"Feature: \",feature)\n",
    "\n",
    "#Create scaler\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "#Scale feature\n",
    "scaled_feature = minmax_scale.fit_transform(feature)\n",
    "\n",
    "print(scaled_feature)\n",
    "\n",
    "# MinMax Scaling new_x = (x-min(x))/(max(x)-min(x))\n",
    "#Rescaling is a common preprocessing task in machine learning.\n",
    "#use fit to calculate the minimum and maximum values of the feature, then use trans form to rescale the feature.\n",
    "#fit_transform does both at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "486011fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[ 1  2]\n",
      " [23  4]\n",
      " [43  5]]\n",
      "Std_X:  [[-1.24371532 -1.33630621]\n",
      " [ 0.0388661   0.26726124]\n",
      " [ 1.20484922  1.06904497]]\n",
      "mean =  0\n",
      "std =  1.0\n"
     ]
    }
   ],
   "source": [
    "# Standardizing a feature\n",
    "# Transform a feature to have a mean of 0 and a standard deviation of 1.\n",
    "np.random.seed(1)\n",
    "#Create feature\n",
    "x =np.array([[1,2],[23,4],[43,5]])\n",
    "\n",
    "#Create a scale\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "#Scaling\n",
    "standardized =scaler.fit_transform(x)\n",
    "\n",
    "print(\"X: \",x)\n",
    "print(\"Std_X: \",standardized)\n",
    "print(\"mean = \",round(np.mean(standardized)))\n",
    "print(\"std = \",np.std(standardized))\n",
    "\n",
    "# new_x = (x-mean(x))/std(x)\n",
    "#As a general rule,use standardization unless you have a specific reason to use an alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f9e38ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.04761905, -1.33333333],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.95238095,  0.66666667]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RobustScaler\n",
    "'''If our data has significant outliers, it can negatively impact our standardization by\n",
    "affecting the feature’s mean and variance. In this scenario, it is often helpful to instead\n",
    "rescale the feature using the median and quartile range.'''\n",
    "\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "\n",
    "robust_scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c1d098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.3 2.4]\n",
      " [4.4 2. ]]\n",
      "Sum of the first observation's values: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Normalizing Observations\n",
    "#rescale the feature values of observations to have unit norm (a total length of 1)\n",
    "'''\n",
    "    The L1 norm that is calculated as the sum of the absolute values of the vector.\n",
    "    The L2 norm that is calculated as the square root of the sum of the squared vector values.\n",
    "    The max norm that is calculated as the maximum vector values.\n",
    "'''\n",
    "\n",
    "features = np.array([[1.3,2.4],[4.4,2.0]])\n",
    "\n",
    "normalizer = preprocessing.Normalizer(norm='l2')\n",
    "print(features)\n",
    "normalizer.transform(features)\n",
    "#Normalizer provides three norm options with Euclidean norm (often called L2)\n",
    "#\"norm='l1' rescales an observation’s values so they sum to 1, which can sometimes be a desirable quality:\"\n",
    "#Manhattan norm (L1)\n",
    "\n",
    "features_l1_norm = preprocessing.Normalizer(norm=\"l1\").transform(features)\n",
    "# Print sum\n",
    "print(\"Sum of the first observation\\'s values:\",\n",
    "features_l1_norm[0, 0] + features_l1_norm[0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bdd433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Polynomial features are often created when we want to include the notion that there\\nexists a nonlinear relationship between the features and the target.\\nThe effects of each feature on the target\\n(sweetness) are dependent on each other. We can encode that relationship by includ‐\\ning an interaction feature that is the product of the individual features.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating Polynomial and interaction features\n",
    "\n",
    "feature = np.array([[1,2],[3,2],[3,6]])\n",
    "\n",
    "polynomial_interaction = preprocessing.PolynomialFeatures(degree=2,include_bias=False)\n",
    "\n",
    "polynomial_interaction.fit_transform(features)\n",
    "\n",
    "interaction = preprocessing.PolynomialFeatures(degree=2,interaction_only=True, include_bias=False)\n",
    "interaction.fit_transform(features)\n",
    "\n",
    "'''Polynomial features are often created when we want to include the notion that there\n",
    "exists a nonlinear relationship between the features and the target.\n",
    "The effects of each feature on the target\n",
    "(sweetness) are dependent on each other. We can encode that relationship by includ‐\n",
    "ing an interaction feature that is the product of the individual features.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4b6f52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 2]\n",
      " [3 6]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2\n",
       "0         11         12\n",
       "1         13         12\n",
       "2         13         16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming Features\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "features = np.array([[1,2],[3,2],[3,6]])\n",
    "\n",
    "def add_ten(x):\n",
    "    return x+10\n",
    "\n",
    "ten_transformer = FunctionTransformer(add_ten)\n",
    "print(features)\n",
    "ten_transformer.transform(features)\n",
    "\n",
    "#This can be done in pandas\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(features,columns=[\"feature_1\",\"feature_2\"])\n",
    "\n",
    "df.apply(add_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa49e8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2]), array([1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detecting the Outliers\n",
    "'''Common Method is to assume the data is normally distributed and based on that assumption\n",
    "“draw” an ellipse around the data, classifying any observation inside the ellipse as an\n",
    "inlier (labeled as 1 ) and any observation outside the ellipse as an outlier (labeled as\n",
    "-1 ):'''\n",
    " \n",
    "import numpy as np\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "#Simulate data\n",
    "features,_ = make_blobs(n_samples=20,\n",
    "                       n_features=2,\n",
    "                       centers=1,\n",
    "                       random_state=1)\n",
    "#Replace the first observation values with extreme values\n",
    "features[0,0]=10000\n",
    "features[0,1]=10000\n",
    "\n",
    "# Create detector\n",
    "outlier_detector = EllipticEnvelope(contamination=.1)\n",
    "# Fit detector\n",
    "outlier_detector.fit(features)\n",
    "# Predict outliers\n",
    "outlier_detector.predict(features)\n",
    "\n",
    "#If we expect our data to have few outliers, we can set contamination to something small.\n",
    "#Instead of looking at observations as a whole, we can instead look at individual features and identify extreme values in those features using interquartile range (IQR):\n",
    "\n",
    "# Create a function to return index of outliers\n",
    "def indicies_of_outliers(x):\n",
    "    q1, q3 = np.percentile(x, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - (iqr * 1.5)\n",
    "    upper_bound = q3 + (iqr * 1.5)\n",
    "    return np.where((x > upper_bound) | (x < lower_bound))\n",
    "\n",
    "# Run function\n",
    "indicies_of_outliers(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ace0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Price  Bathrooms  Square_Feet\n",
      "0  534433        2.0         1500\n",
      "1  392333        3.5         2500\n",
      "2  293222        2.0         1500\n",
      "     Price  Bathrooms  Square_Feet  Outlier\n",
      "0   534433        2.0         1500        0\n",
      "1   392333        3.5         2500        0\n",
      "2   293222        2.0         1500        0\n",
      "3  4322032      116.0        48000        1\n",
      "     Price  Bathrooms  Square_Feet  Outlier  Log_Of_Square_Feet\n",
      "0   534433        2.0         1500        0            7.313220\n",
      "1   392333        3.5         2500        0            7.824046\n",
      "2   293222        2.0         1500        0            7.313220\n",
      "3  4322032      116.0        48000        1           10.778956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'if you do have outliers standardization might not be appropri‐\\nate because the mean and variance might be highly influenced by the outliers. In this\\ncase, use a rescaling method more robust against outliers like RobustScaler .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling Outliers\n",
    "'''First, we should consider what\n",
    "makes them an outlier. If we believe they are errors in the data such as from a broken\n",
    "sensor or a miscoded value, then we might drop the observation or replace outlier\n",
    "values with NaN. if we believe the outliers\n",
    "are genuine extreme values (e.g., a house [mansion] with 200 bathrooms), then mark‐\n",
    "ing them as outliers or transforming their values is more appropriate.'''\n",
    "\n",
    "'''Second, how we handle outliers should be based on our goal for machine learning.'''\n",
    "\n",
    "houses = pd.DataFrame()\n",
    "houses[\"Price\"]=[534433, 392333, 293222, 4322032]\n",
    "houses['Bathrooms'] = [2, 3.5, 2, 116]\n",
    "houses['Square_Feet'] = [1500, 2500, 1500, 48000]\n",
    "\n",
    "# Filter observations \n",
    "print(houses[houses['Bathrooms'] < 20]) #We have removed the bathroom of 116\n",
    "\n",
    "# Create feature based on boolean condition\n",
    "houses[\"Outlier\"] = np.where(houses[\"Bathrooms\"] < 20, 0, 1)\n",
    "\n",
    "print(houses)\n",
    "\n",
    "# Log feature\n",
    "houses[\"Log_Of_Square_Feet\"] = [np.log(x) for x in houses[\"Square_Feet\"]]\n",
    "print(houses)\n",
    "\n",
    "'''if you do have outliers standardization might not be appropri‐\n",
    "ate because the mean and variance might be highly influenced by the outliers. In this\n",
    "case, use a rescaling method more robust against outliers like RobustScaler .'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93b39393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44 19 26 47 11 38 53 51 67 35]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass threshold=18 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class intervals or Discretizating Features\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "age = np.array([[44, 19, 26, 47, 11, 38, 53, 51, 67, 35]])\n",
    "print(age)\n",
    "binarizer = Binarizer(18)\n",
    "binarizer.fit_transform(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "888e6f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44 19 26 47 11 38 53 51 67 35]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 1, 2, 0, 2, 2, 2, 3, 2]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can break up numerical features according to multiple thresholds:\n",
    "#Bin feature\n",
    "print(age)\n",
    "np.digitize(age,bins=[20,30,68])\n",
    "\n",
    "#bins parameter denote the left edge of each bin\n",
    "#We can switch this behavior by setting the parameter right to True :\n",
    "    \n",
    "np.digitize(age, bins=[20,30,64], right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "647371a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9.877554</td>\n",
       "      <td>-3.336145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.287210</td>\n",
       "      <td>-8.353986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.943061</td>\n",
       "      <td>-7.023744</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.440167</td>\n",
       "      <td>-8.791959</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.641388</td>\n",
       "      <td>-8.075888</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  group\n",
       "0  -9.877554  -3.336145      0\n",
       "1  -7.287210  -8.353986      2\n",
       "2  -6.943061  -7.023744      2\n",
       "3  -7.440167  -8.791959      2\n",
       "4  -6.641388  -8.075888      2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping Observations Using Clustering\n",
    "#You want to cluster observations so that similar observations are grouped together.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "features,_=make_blobs(n_samples=50,\n",
    "                     n_features=2,\n",
    "                     centers=3,\n",
    "                     random_state=1)\n",
    "#create dataset\n",
    "dataframe = pd.DataFrame(features,columns=[\"feature_1\",\"feature_2\"])\n",
    "\n",
    "#K-Means cluster\n",
    "clusterer = KMeans(3,random_state=0)\n",
    "\n",
    "#Fit clusterer\n",
    "clusterer.fit(features)\n",
    "\n",
    "#Predict values\n",
    "dataframe['group'] =clusterer.predict(features)\n",
    "\n",
    "dataframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c79f7599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3. nan]\n",
      " [ 3.  4.  5.  6.]\n",
      " [ 7.  5. 23. nan]]\n",
      "[[3. 4. 5. 6.]]\n",
      "   age  name\n",
      "0  1.0   dfs\n",
      "1  2.0   NaN\n",
      "2  3.0   fer\n",
      "3  4.0   sdf\n",
      "4  NaN  sfdg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>dfs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>fer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>sdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age name\n",
       "0  1.0  dfs\n",
       "2  3.0  fer\n",
       "3  4.0  sdf"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deleting Observations with Missing Values\n",
    "import numpy as np\n",
    "\n",
    "features = np.array([[1,2,3,np.nan],[3,4,5,6],[7,5,23,np.nan]])\n",
    "print(features)\n",
    "#Keep only observations that are not (denoted by ~)missing\n",
    "print(features[~np.isnan(features).any(axis=1)])\n",
    "\n",
    "# Pandas.dropna() can also be used\n",
    "# Load data\n",
    "dataframe = pd.DataFrame({\"age\":[1,2,3,4,np.nan],\"name\":[\"dfs\",np.nan,\"fer\",\"sdf\",\"sfdg\"]})\n",
    "print(dataframe)\n",
    "# Remove observations with missing values\n",
    "dataframe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "864230eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three types of missing data:\\nMissing Completely At Random (MCAR)\\nThe probability that a value is missing is independent of everything. For example,\\na survey respondent rolls a die before answering a question: if she rolls a six, she\\nskips that question.\\nMissing At Random (MAR)\\nThe probability that a value is missing is not completely random, but depends on\\nthe information captured in other features. For example, a survey asks about gen‐\\nder identity and annual salary and women are more likely to skip the salary ques‐\\ntion; however, their nonresponse depends only on information we have captured\\nin our gender identity feature.\\nMissing Not At Random (MNAR)\\nThe probability that a value is missing is not random and depends on informa‐\\ntion not captured in our features. For example, a survey asks about gender iden‐\\ntity and women are more likely to skip the salary question, and we do not have a\\ngender identity feature in our data.\\nIt is sometimes acceptable to delete observations if they are MCAR or MAR. How‐\\never, if the value is MNAR, the fact that a value is missing is itself information. Delet‐\\ning MNAR observations can inject bias into our data because we are removing obser‐\\nvations produced by some unobserved systematic effect.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''There are three types of missing data:\n",
    "Missing Completely At Random (MCAR)\n",
    "The probability that a value is missing is independent of everything. For example,\n",
    "a survey respondent rolls a die before answering a question: if she rolls a six, she\n",
    "skips that question.\n",
    "Missing At Random (MAR)\n",
    "The probability that a value is missing is not completely random, but depends on\n",
    "the information captured in other features. For example, a survey asks about gen‐\n",
    "der identity and annual salary and women are more likely to skip the salary ques‐\n",
    "tion; however, their nonresponse depends only on information we have captured\n",
    "in our gender identity feature.\n",
    "Missing Not At Random (MNAR)\n",
    "The probability that a value is missing is not random and depends on informa‐\n",
    "tion not captured in our features. For example, a survey asks about gender iden‐\n",
    "tity and women are more likely to skip the salary question, and we do not have a\n",
    "gender identity feature in our data.\n",
    "It is sometimes acceptable to delete observations if they are MCAR or MAR. How‐\n",
    "ever, if the value is MNAR, the fact that a value is missing is itself information. Delet‐\n",
    "ing MNAR observations can inject bias into our data because we are removing obser‐\n",
    "vations produced by some unobserved systematic effect.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a310af36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Value: 0.8730186113995938\n",
      "Imputed Value: 1.0955332713113226\n"
     ]
    }
   ],
   "source": [
    "# Imputing Missing values\n",
    "#If you have a small amount of data, predict the missing values using k-nearest neighbors (KNN):\n",
    "\n",
    "import numpy as np\n",
    "from fancyimpute import KNN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "features,_=make_blobs(n_samples=1000,\n",
    "                     n_features=2,\n",
    "                     random_state=1)\n",
    "#Standardize the features\n",
    "scaler = StandardScaler()\n",
    "standardized_features = scaler.fit_transform(features)\n",
    "\n",
    "#Replace the first feature's first value with a missing value\n",
    "true_value = standardized_features[0,0]\n",
    "standardized_features[0,0]=np.nan\n",
    "\n",
    "# Predict the missing values in the feature matrix\n",
    "features_knn_imputed = KNN(k=5,verbose=0).fit_transform(standardized_features)\n",
    "\n",
    "# Compare true and imputed values\n",
    "print(\"True Value:\", true_value)\n",
    "print(\"Imputed Value:\", features_knn_imputed[0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0afbcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Value: 0.8730186113995938\n",
      "Imputed Value: -3.058372724614996\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Alternatively, we can use scikit-learn’s Imputer module to fill in missing values with\n",
    "the feature’s mean, median, or most frequent value. However, we will typically get\n",
    "worse results than KNN:\"\"\"\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "#Create imputer\n",
    "mean_imputer = SimpleImputer(strategy=\"mean\",missing_values=np.nan)\n",
    "#Impute values\n",
    "\n",
    "features_mean_imputed = mean_imputer.fit_transform(features)\n",
    "\n",
    "# Compare true and imputed values\n",
    "print(\"True Value:\", true_value)\n",
    "print(\"Imputed Value:\", features_mean_imputed[0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7004644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The downside to KNN is that in order to know which observations are the closest to\\nthe missing value, it needs to calculate the distance between the missing value and\\nevery single observation. This is reasonable in smaller datasets, but quickly becomes\\nproblematic if a dataset has millions of observations.\\nAn alternative and more scalable strategy is to fill in all missing values with some\\naverage value. For example, in our solution we used scikit-learn to fill in missing val‐\\nues with a feature’s mean value. The imputed value is often not as close to the true\\nvalue as when we used KNN, but we can scale mean-filling to data containing mil‐\\nlions of observations easily.\\nIf we use imputation, it is a good idea to create a binary feature indicating whether or\\nnot the observation contains an imputed value.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The downside to KNN is that in order to know which observations are the closest to\n",
    "the missing value, it needs to calculate the distance between the missing value and\n",
    "every single observation. This is reasonable in smaller datasets, but quickly becomes\n",
    "problematic if a dataset has millions of observations.\n",
    "An alternative and more scalable strategy is to fill in all missing values with some\n",
    "average value. For example, in our solution we used scikit-learn to fill in missing val‐\n",
    "ues with a feature’s mean value. The imputed value is often not as close to the true\n",
    "value as when we used KNN, but we can scale mean-filling to data containing mil‐\n",
    "lions of observations easily.\n",
    "If we use imputation, it is a good idea to create a binary feature indicating whether or\n",
    "not the observation contains an imputed value.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
