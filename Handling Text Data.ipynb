{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7850460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8729674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  Hello Myself  ', 'Cooking is my skill  ', 'You should consider cooking as a   good. Habit']\n",
      "Stripped text : ['Hello Myself', 'Cooking is my skill', 'You should consider cooking as a   good. Habit']\n"
     ]
    }
   ],
   "source": [
    "# Removing whitespaces \n",
    "text = [\"  Hello Myself  \",\"Cooking is my skill  \",\"You should consider cooking as a   good. Habit\"]\n",
    "\n",
    "#Strip Whitespaces\n",
    "strip_space = [string.strip() for string in text]\n",
    "print(text)\n",
    "print(\"Stripped text :\",strip_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1698e6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Periods : ['Hello Myself', 'Cooking is my skill', 'You should consider cooking as a   good Habit']\n"
     ]
    }
   ],
   "source": [
    "# Remove perionds\n",
    "remove_periods = [string.replace(\".\",\"\") for string in strip_space]\n",
    "print(\"Removed Periods :\",remove_periods) # It replaces the full stop with space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31ccace4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HELLO MYSELF',\n",
       " 'COOKING IS MY SKILL',\n",
       " 'YOU SHOULD CONSIDER COOKING AS A   GOOD HABIT']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using functions\n",
    "def capitalizer(string: str) -> str:\n",
    "    return string.upper()\n",
    "\n",
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "026f338f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXX XXXXXX',\n",
       " 'XXXXXXX XX XX XXXXX',\n",
       " 'XXX XXXXXX XXXXXXXX XXXXXXX XX X   XXXX XXXXX']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regular expressions library\n",
    "import re\n",
    "\n",
    "def replace_letters_with_X(string: str)-> str:\n",
    "    return re.sub(r\"[a-zA-Z]\",\"X\",string)\n",
    "\n",
    "[replace_letters_with_X(string) for string in remove_periods]\n",
    "    \n",
    "\"\"\"In the\n",
    "real world we will most likely define a custom cleaning function (e.g., capitalizer )\n",
    "combining some cleaning tasks and apply that to the text data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dd36dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.10.0-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 725 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.1-py3-none-any.whl (37 kB)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=81384c312a5a53e3ac484168e9cef91d1641abf8d5615a33715b302a5f5ab6b2\n",
      "  Stored in directory: /home/kirankumar/.cache/pip/wheels/75/78/21/68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "Successfully built bs4\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.10.0 bs4-0.0.1 soupsieve-2.3.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d06ee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><body><div class=\"full_name\"><span style=\"font-weight:bold\">\n",
      "Masego</span> Azra</div>\" \n",
      "</body></html>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nMasego Azra'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsing and Cleaning HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Sample HTML\n",
    "html = \"\"\"\n",
    "<div class='full_name'><span style='font-weight:bold'>\n",
    "Masego</span> Azra</div>\" \n",
    "\"\"\"\n",
    "#Parse html\n",
    "soup = BeautifulSoup(html,\"lxml\")\n",
    "\n",
    "#Find the div with class \"full_name\", show text\n",
    "soup.find(\"div\",{\"class\":\"full_name\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5560f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Removing Punctions\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "text = ['Hi!!!! I. Love. This. Song....',\n",
    "'10000% Agree!!!! #LoveIT',\n",
    "'Right?!?!']\n",
    "\n",
    "#create a dictionary of punctuation characters\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                           if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "[string.translate(punctuation) for string in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c18eecc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntranslate is a Python method popular due to its blazing speed. In our solution, first\\nwe created a dictionary, punctuation , with all punctuation characters according to\\nUnicode as its keys and None as its values. Next we translated all characters in the\\nstring that are in punctuation into None , effectively removing them. There are more\\nreadable ways to remove punctuation, but this somewhat hacky solution has the\\nadvantage of being far faster than alternatives.\\nIt is important to be conscious of the fact that punctuation contains information (e.g.,\\n“Right?” versus “Right!”). Removing punctuation is often a necessary evil to create\\nfeatures; however, if the punctuation is important we should make sure to take that\\ninto account\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "translate is a Python method popular due to its blazing speed. In our solution, first\n",
    "we created a dictionary, punctuation , with all punctuation characters according to\n",
    "Unicode as its keys and None as its values. Next we translated all characters in the\n",
    "string that are in punctuation into None , effectively removing them. There are more\n",
    "readable ways to remove punctuation, but this somewhat hacky solution has the\n",
    "advantage of being far faster than alternatives.\n",
    "It is important to be conscious of the fact that punctuation contains information (e.g.,\n",
    "“Right?” versus “Right!”). Removing punctuation is often a necessary evil to create\n",
    "features; however, if the punctuation is important we should make sure to take that\n",
    "into account\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ac82bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'will', 'go', 'to', 'Japan', 'for', 'business', 'trip']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing Text\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string = \"I will go to Japan for business trip\"\n",
    "\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fd52d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize senyences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# Create text\n",
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
    "# Tokenize sentences\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e33fbb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'go', 'Japan', 'business', 'trip']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Stop Words\n",
    "#First download stopwords\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenize_words = ['I', 'will', 'go', 'to', 'Japan', 'for', 'business', 'trip']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "[word for word in tokenize_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "818d1550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Note that NLTK’s stopwords assumes the tokenized words are all lowercased.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Note that NLTK’s stopwords assumes the tokenized words are all lowercased.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21f2cf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While “stop words” can refer to any set of words we want to remove before process‐\\ning, frequently the term refers to extremely common words that themselves contain\\nlittle information value. NLTK has a list of common stop words that we can use to\\nfind and remove stop words in our tokenized words\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''While “stop words” can refer to any set of words we want to remove before process‐\n",
    "ing, frequently the term refers to extremely common words that themselves contain\n",
    "little information value. NLTK has a list of common stop words that we can use to\n",
    "find and remove stop words in our tokenized words\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2882428d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming words\n",
    "# To convert tokenized words to their root forms\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a528cb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Diana', 'NNP'), ('likes', 'VBZ'), ('playing', 'VBG'), ('cricket', 'NN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tagging Parts of Speech\n",
    "#import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text_data = \"Diana likes playing cricket\"\n",
    "\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4194c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTag  Part of speech\\nNNP  Proper noun, singular\\nNN   Noun, singular or mass\\nRB   Adverb\\nVBD  Verb, past tense\\nVBG  Verb, gerund or present participle\\nJJ   Adjective\\nPRP  Personal pronoun\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NLTK uses the Penn Treebank parts for speech tags\n",
    "\n",
    "Tag  Part of speech\n",
    "NNP  Proper noun, singular\n",
    "NN   Noun, singular or mass\n",
    "RB   Adverb\n",
    "VBD  Verb, past tense\n",
    "VBG  Verb, gerund or present participle\n",
    "JJ   Adjective\n",
    "PRP  Personal pronoun\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9102ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Diana', 'cricket']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter words\n",
    "[word for word, tag in text_tagged if tag in ['NN','NNS','NNP','NNPS'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c05d2af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labeling the parts of speech in tweets\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    "\"Political science is an amazing field\",\n",
    "\"San Francisco is an awesome city\"]\n",
    "\n",
    "tagged_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet_tag = pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "\n",
    "#Using one-hot encoding\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8f49931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show feature names\n",
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525119a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8174734002697437"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train your own tagger using Brown Corpus\n",
    "#import nltk\n",
    "#nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "# Get some text from the Brown Corpus, broken into sentences\n",
    "sentences = brown.tagged_sents(categories=\"news\")\n",
    "\n",
    "# Split into 4000 sentences for training and 623 for testing\n",
    "train = sentences[:4000]\n",
    "test = sentences[4000:]\n",
    "# Create backoff tagger\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigram)\n",
    "# Show accuracy\n",
    "trigram.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c60879f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding Text as a Bag of words\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = np.array(['I love Brazil. Brazil!',\n",
    "'Sweden is best',\n",
    "'Germany beats both'])\n",
    "\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text)\n",
    "\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e6a67a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf52a500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show feature names\n",
    "count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CountVectorizer comes with a number of useful parameters to make creating bag-\n",
    "of-words feature matrices easy. First, while by default every feature is a word, that\n",
    "does not have to be the case. Instead we can set every feature to be the combination of\n",
    "two words (called a 2-gram) or even three words (3-gram). ngram_range sets the\n",
    "minimum and maximum size of our n-grams. For example, (2,3) will return all 2-\n",
    "grams and 3-grams. Second, we can easily remove low-information filler words using\n",
    "stop_words either with a built-in list or a custom list. Finally, we can restrict the\n",
    "words or phrases we want to consider to a certain list of words using vocabulary . For\n",
    "example, we could create a bag-of-words feature matrix for only occurrences of coun‐\n",
    "try names:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "590efac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [0]\n",
      " [0]]\n",
      "{'brazil': 0}\n"
     ]
    }
   ],
   "source": [
    "# Create feature matrix with arguments\n",
    "count_2gram = CountVectorizer(ngram_range=(1,2),\n",
    "stop_words=\"english\",\n",
    "vocabulary=['brazil'])\n",
    "\n",
    "bag = count_2gram.fit_transform(text)\n",
    "# View feature matrix\n",
    "print(bag.toarray())\n",
    "\n",
    "# View the 1-grams and 2-grams\n",
    "print(count_2gram.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
